import os
import numpy as np
import pandas as pd
import streamlit as st
import altair as alt
from datetime import timedelta
from tensorflow.keras.models import load_model
import joblib

# ========== PAGE SETUP ==========
TIME_COL     = "Date"
CLUSTER_COL  = "geo_cluster"                 # d√πng cluster, kh√¥ng c√≤n station_id
TARGET_COL   = "estimated_demand_kWh"
EXOG_COLS    = ["public_holiday","school_holiday","is_weekend",
                "Avg_Temp","Avg_Humidity","Avg_Wind"]

CLUSTER_ARTIFACT_ROOT = "artifacts"
st.set_page_config(page_title="EVAT ‚Äî GRU Forecast by Cluster", page_icon="‚ö°", layout="wide")
st.title("‚ö° EVAT ‚Äî GRU Forecast per Cluster")
st.caption("Select a cluster, adjust external factors ‚Üí get a **forecast line chart**.")

# ========== UTILITY FUNCTIONS ==========
@st.cache_data(show_spinner=False)
def load_history(path: str) -> pd.DataFrame:
    """
    Y√™u c·∫ßu c·ªôt: Date, geo_cluster, estimated_demand_kWh, c√πng c√°c EXOG_COLS.
    """
    df = pd.read_csv(path, parse_dates=[TIME_COL])
    df = df.sort_values([CLUSTER_COL, TIME_COL]).reset_index(drop=True)
    needed = [TIME_COL, CLUSTER_COL, TARGET_COL] + EXOG_COLS
    miss = [c for c in needed if c not in df.columns]
    if miss:
        raise ValueError(f"`{path}` thi·∫øu c·ªôt: {miss}")
    # √©p ki·ªÉu geo_cluster v√† lo·∫°i NaN/√¢m
    df[CLUSTER_COL] = pd.to_numeric(df[CLUSTER_COL], errors="coerce")
    if df[CLUSTER_COL].isna().any():
        raise ValueError(f"C√≥ NaN ·ªü `{CLUSTER_COL}` trong {path}. Vui l√≤ng l√†m s·∫°ch d·ªØ li·ªáu.")
    if (df[CLUSTER_COL] < 0).any():
        raise ValueError(f"C√≥ gi√° tr·ªã √¢m ·ªü `{CLUSTER_COL}` (v√≠ d·ª• -1). Vui l√≤ng l·ªçc b·ªè.")
    return df

def cluster_dir_candidates(cid: int) -> list:
    cid = int(cid)
    return [
        os.path.join("artifacts", "clusters", str(cid)),   # c·∫•u tr√∫c m·ªõi
        os.path.join("artifacts", f"cluster_{cid}"),       # ph√≤ng c·∫•u tr√∫c c≈©
    ]

@st.cache_resource(show_spinner=False)
def load_artifacts_for_cluster(geo_cluster: int):
    tried = []
    for cdir in cluster_dir_candidates(geo_cluster):
        mpath = os.path.join(cdir, "model_gru.keras")
        spath = os.path.join(cdir, "scaler_all.joblib")
        tpath = os.path.join(cdir, "tail.npy")
        tried.append((cdir, mpath, spath, tpath))
        if os.path.exists(mpath) and os.path.exists(spath):
            st.write("‚úÖ Using cluster artifacts from:", cdir)
            try:
                st.write("Contents:", os.listdir(cdir))
            except Exception:
                pass
            model = load_model(mpath)
            scaler = joblib.load(spath)
            tail_scaled = np.load(tpath) if os.path.exists(tpath) else None
            # tr·∫£ v·ªÅ SEQ_LEN, N_FEAT t·ª´ input shape model
            return model, scaler, tail_scaled, model.input_shape[1], model.input_shape[2]

    st.write("‚ùå Could not find artifacts for cluster:", geo_cluster)
    st.write("CWD:", os.getcwd())
    for cdir, mpath, spath, _ in tried:
        st.write("Tried:", cdir,
                 "| model exists?", os.path.exists(mpath),
                 "| scaler exists?", os.path.exists(spath))
        if os.path.exists(cdir):
            try:
                st.write("Dir contents:", os.listdir(cdir))
            except Exception:
                pass
    raise FileNotFoundError(
        f"Kh√¥ng t√¨m th·∫•y model/scaler cho c·ª•m {geo_cluster}. "
        f"Y√™u c·∫ßu c√°c file 'model_gru.keras', 'scaler_all.joblib' (v√† t√πy ch·ªçn 'tail.npy') "
        f"trong: {', '.join(cluster_dir_candidates(geo_cluster))}"
    )

def build_feature_matrix(df: pd.DataFrame) -> pd.DataFrame:
    cols = [TIME_COL, CLUSTER_COL, TARGET_COL] + EXOG_COLS
    return df[cols].copy()

def _scale_matrix_like_training(mat: np.ndarray, scaler) -> np.ndarray:
    """
    N·∫øu scaler ƒë√£ fit theo c·ªôt (7 c·ªôt) -> transform tr·ª±c ti·∫øp (T,7).
    N·∫øu scaler 1-c·ªôt (flatten) -> gi·ªØ nguy√™n c√°ch c≈© ƒë·ªÉ tr√°nh l·ªách ph√¢n ph·ªëi.
    """
    n_in = getattr(scaler, "n_features_in_", None)
    if n_in == mat.shape[1]:           # v√≠ d·ª• 7 c·ªôt: [target] + 6 exog
        return scaler.transform(mat)
    # --- fallback: scaler 1-c·ªôt (c√°ch c≈©) ---
    h, w = mat.shape
    flat = mat.reshape(-1, 1)
    flat_scaled = scaler.transform(flat)
    return flat_scaled.reshape(h, w)

def _inverse_vector_like_training(vec: np.ndarray, scaler) -> np.ndarray:
    """
    Inverse target theo ƒë√∫ng ki·ªÉu scaler ƒë√£ fit.
    - Feature-wise (7 c·ªôt): d√πng min_/scale_ c·ªßa c·ªôt target (index 0).
    - 1-c·ªôt (flatten): d√πng inverse_transform nh∆∞ c≈©.
    """
    n_in = getattr(scaler, "n_features_in_", None)
    if n_in and n_in > 1:
        # MinMax inverse cho c·ªôt 0: X = (X_scaled - min_[0]) / scale_[0]
        return (vec - scaler.min_[0]) / scaler.scale_[0]
    # --- fallback: scaler 1-c·ªôt ---
    flat = vec.reshape(-1, 1)
    inv = scaler.inverse_transform(flat)
    return inv.reshape(-1)

def take_last_sequence_scaled(df_feat: pd.DataFrame, geo_cluster, seq_len: int, scaler) -> np.ndarray:
    """
    L·∫•y SEQ_LEN cu·ªëi c·ªßa c·ª•m ‚Üí (seq_len, n_feat), r·ªìi scale theo c√°ch flatten-1-c·ªôt.
    """
    d = df_feat[df_feat[CLUSTER_COL] == geo_cluster].tail(seq_len)
    if len(d) < seq_len:
        raise ValueError(f"L·ªãch s·ª≠ cho c·ª•m {geo_cluster} < SEQ_LEN={seq_len}.")
    mat = d[[TARGET_COL] + EXOG_COLS].to_numpy().astype(float)
    return _scale_matrix_like_training(mat, scaler)

def make_future_exog_overrides(base_row: pd.Series, horizon: int, overrides: dict) -> pd.DataFrame:
    rows = []
    for _ in range(horizon):
        r = {c: base_row.get(c, np.nan) for c in EXOG_COLS}
        r.update(overrides)
        rows.append(r)
    return pd.DataFrame(rows)

def scale_future_exog(future_exog_df: pd.DataFrame, scaler, n_feat: int) -> np.ndarray:
    ex = future_exog_df[EXOG_COLS].to_numpy().astype(float)
    ex_scaled = _scale_matrix_like_training(ex, scaler)
    return ex_scaled

def recursive_forecast(model, scaler, seed_scaled: np.ndarray, exog_future_scaled: np.ndarray, horizon: int) -> np.ndarray:
    seq_len, n_feat = seed_scaled.shape
    seq = seed_scaled.copy()
    out_scaled = []
    for t in range(horizon):
        x = seq[-seq_len:].reshape(1, seq_len, n_feat)
        yhat_scaled = model.predict(x, verbose=0).ravel()[0]
        next_vec = np.empty((n_feat,), dtype=float)
        next_vec[0] = yhat_scaled
        next_vec[1:] = exog_future_scaled[t]
        seq = np.vstack([seq, next_vec])
        out_scaled.append(yhat_scaled)
    yhat_scaled_arr = np.array(out_scaled, dtype=float)
    inv = _inverse_vector_like_training(yhat_scaled_arr, scaler)
    return inv

def infer_freq(ts: pd.Series) -> pd.Timedelta:
    diffs = ts.diff()
    if diffs.notna().any():
        return diffs.mode().iloc[0] if not diffs.mode().empty else diffs.median()
    return pd.Timedelta(hours=1)

# ==========/ SIDEBAR ==========
st.sidebar.subheader("Data path")
hist_path = st.sidebar.text_input("cluster_history.csv", "cluster_history.csv")

st.sidebar.subheader("External factors (override)")
ph = st.sidebar.selectbox("Public holiday", [0, 1], index=0)
sh = st.sidebar.selectbox("School holiday", [0, 1], index=0)
we = st.sidebar.selectbox("Weekend", [0, 1], index=0)
t_avg = st.sidebar.slider("Avg_Temp (¬∞C)", -5.0, 45.0, 24.0, 0.5)
h_avg = st.sidebar.slider("Avg_Humidity (%)", 0.0, 100.0, 60.0, 1.0)
w_avg = st.sidebar.slider("Avg_Wind (m/s)", 0.0, 20.0, 3.0, 0.2)

# ==========/ LOAD ==========
hist_path = "cluster_history.csv"

df_hist = load_history(hist_path)

with st.expander("üëÄ Xem to√†n b·ªô cluster_history.csv"):
    st.dataframe(df_hist, use_container_width=True)

if TARGET_COL not in df_hist.columns:
    st.error(f"Kh√¥ng t√¨m th·∫•y c·ªôt {TARGET_COL} trong {hist_path}")
    st.stop()

allowed_clusters = [0, 1, 2, 3, 4]
clusters = [c for c in allowed_clusters if c in df_hist[CLUSTER_COL].unique()]
geo_cluster = st.selectbox("Cluster", clusters)

# Artifacts theo C·ª§M
model, scaler, tail_scaled_opt, SEQ_LEN, N_FEAT = load_artifacts_for_cluster(int(geo_cluster))

# Ki·ªÉm tra scaler & n_feat
n_in = getattr(scaler, "n_features_in_", None)
expected_feats = 1 + len(EXOG_COLS)
if n_in is not None and n_in != 1:
    st.error(f"Scaler cho c·ª•m {geo_cluster} c√≥ n_features_in_={n_in}, "
             f"trong khi pipeline d√πng scaler 1-c·ªôt. H√£y export scaler ƒë√∫ng.")
    st.stop()
if N_FEAT != expected_feats:
    st.error(f"Model N_FEAT={N_FEAT} nh∆∞ng app mong ƒë·ª£i {expected_feats} "
             f"(1 target + {len(EXOG_COLS)} exog). Ki·ªÉm tra l·∫°i model c·ª•m.")
    st.stop()

# Nh·∫≠n d·∫°ng lo·∫°i model theo output shape v√† ·∫§N ƒê·ªäNH HORIZON (kh√¥ng c·∫ßn nh·∫≠p)
out_units = (model.output_shape[-1] if isinstance(model.output_shape, tuple)
             else model.output_shape[0][-1])
is_direct_multi_output = out_units > 1  # v√≠ d·ª• = 14 theo code train c·ªßa b·∫°n

if is_direct_multi_output:
    final_horizon = out_units
    st.caption(f"üìè Horizon c·ªë ƒë·ªãnh theo m√¥ h√¨nh: **{final_horizon}** b∆∞·ªõc.")
else:
    # N·∫øu model 1-b∆∞·ªõc, ta ·∫•n ƒë·ªãnh m·∫∑c ƒë·ªãnh 14 b∆∞·ªõc ƒë·ªÉ gi·ªØ h√†nh vi quen thu·ªôc.
    final_horizon = 14  # <-- ƒë·ªïi s·ªë n√†y n·∫øu b·∫°n mu·ªën m·∫∑c ƒë·ªãnh kh√°c
    st.caption(f"üìè Model 1-b∆∞·ªõc: d√πng horizon m·∫∑c ƒë·ªãnh **{final_horizon}** (kh√¥ng c√≥ √¥ nh·∫≠p).")

# ==========/ SEED ==========
# ==========/ SEED ==========
df_feat = build_feature_matrix(df_hist)

# L·∫•y 50 b∆∞·ªõc cu·ªëi c·ªßa c·ª•m l√†m seed (gi·ªØ nguy√™n target history)
seed_raw = (
    df_feat[df_feat[CLUSTER_COL] == geo_cluster]
    .sort_values(TIME_COL)
    .tail(SEQ_LEN)
    .copy()
)
if len(seed_raw) < SEQ_LEN:
    st.error(f"L·ªãch s·ª≠ cho c·ª•m {geo_cluster} < SEQ_LEN={SEQ_LEN}.")
    st.stop()

# üëâ GHI ƒê√à EXOG TRONG C·ª¨A S·ªî B·∫∞NG GI√Å TR·ªä USER CH·ªåN
seed_raw.loc[:, "public_holiday"] = int(ph)
seed_raw.loc[:, "school_holiday"] = int(sh)
seed_raw.loc[:, "is_weekend"] = int(we)
seed_raw.loc[:, "Avg_Temp"] = float(t_avg)
seed_raw.loc[:, "Avg_Humidity"] = float(h_avg)
seed_raw.loc[:, "Avg_Wind"] = float(w_avg)

# Scale seed theo c√°ch flatten-1-c·ªôt (ƒë√∫ng pipeline train)
seed_mat = seed_raw[[TARGET_COL] + EXOG_COLS].to_numpy().astype(float)
seed_scaled = _scale_matrix_like_training(seed_mat, scaler)

# ==========/ FORECAST ==========
if is_direct_multi_output:
    # D·ª± b√°o tr·ª±c ti·∫øp H b∆∞·ªõc t·ª´ seed ƒë√£ override EXOG
    x_in = seed_scaled.reshape(1, SEQ_LEN, N_FEAT)
    yhat_scaled = model.predict(x_in, verbose=0).reshape(-1)      # (H,)
    yhat = _inverse_vector_like_training(yhat_scaled, scaler)     # v·ªÅ kWh
else:
    # Model 1-b∆∞·ªõc (√≠t g·∫∑p trong code train c·ªßa b·∫°n) - v·∫´n h·ªó tr·ª£
    # t·∫°o exog_future l·∫∑p l·∫°i ƒë√∫ng c√°c gi√° tr·ªã user ƒë·ªÉ nh·∫•t qu√°n
    overrides = {
        "public_holiday": int(ph),
        "school_holiday": int(sh),
        "is_weekend": int(we),
        "Avg_Temp": float(t_avg),
        "Avg_Humidity": float(h_avg),
        "Avg_Wind": float(w_avg),
    }
    last_row = seed_raw.tail(1).iloc[0]
    future_exog = make_future_exog_overrides(last_row, final_horizon, overrides)
    exog_future_scaled = scale_future_exog(future_exog, scaler, N_FEAT)
    yhat = recursive_forecast(model, scaler, seed_scaled, exog_future_scaled, horizon=final_horizon)

# ==========/ PLOT ==========
hist_tail = df_hist[df_hist[CLUSTER_COL] == geo_cluster].sort_values(TIME_COL).tail(SEQ_LEN).copy()
t0 = hist_tail[TIME_COL].iloc[-1]
freq = infer_freq(hist_tail[TIME_COL])
future_times = [t0 + (i+1)*freq for i in range(final_horizon)]

df_plot_hist = pd.DataFrame({
    "timestamp": hist_tail[TIME_COL],
    "value": hist_tail[TARGET_COL],
    "type": "History"
})
df_plot_fcst = pd.DataFrame({
    "timestamp": future_times,
    "value": yhat,
    "type": "Forecast"
})
df_plot = pd.concat([df_plot_hist, df_plot_fcst], ignore_index=True)

chart = alt.Chart(df_plot).mark_line().encode(
    x=alt.X("timestamp:T", title="Time"),
    y=alt.Y("value:Q", title="Demand (kWh)"),
    color=alt.Color("type:N", sort=["History","Forecast"])
).properties(width="container", height=380,
             title=f"Cluster {geo_cluster} ‚Äî GRU Forecast ({final_horizon} steps)")
st.altair_chart(chart, use_container_width=True)

# ==========/ EXPORT ==========
with st.expander("Export"):
    st.download_button(
        "Download Forecast CSV",
        data=df_plot_fcst.to_csv(index=False),
        file_name=f"forecast_cluster_{geo_cluster}.csv",
        mime="text/csv"
    )
